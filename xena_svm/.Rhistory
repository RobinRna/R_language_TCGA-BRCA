cat("警告: 最小类别样本数 (", min_class_count_train, ") < CV折叠数 (", train_control_svm$number, ").\n")
}
caret_svm_model <- NULL
tryCatch({
caret_svm_model <- train(label ~ ., data = train_set,
method = "svmRadial",      # RBF核SVM (kernlab或e1071)
trControl = train_control_svm,
tuneGrid = tune_grid_svm_radial,
metric = "Mean_F1",        # 用Mean F1选择最优模型
preProcess = c("center", "scale"), # 标准化特征
na.action = na.omit
# prob.model = TRUE # 对于kernlab的ksvm，确保概率模型被训练 (如果classProbs=TRUE)
)
}, error = function(e) {
cat("错误: caret SVM模型训练失败: ", conditionMessage(e), "\n"); caret_svm_model <<- NULL
})
if (is.null(caret_svm_model)) stop("caret SVM模型未能成功训练。")
cat("caret SVM模型训练和调优完成。\n")
# --- 6. 查看、记录和绘制调优结果 ---
cat("\n步骤4: 查看、记录和绘制SVM调优结果...\n")
cat("最优超参数组合 (Best Tune):\n"); print(caret_svm_model$bestTune)
cat("\n调优过程中的性能指标概览 (部分):\n"); print(head(caret_svm_model$results))
# 准备绘图数据 (caret$results已经是长格式，但可能需要调整用于ggplot)
results_to_plot_svm <- caret_svm_model$results
# 绘制调优曲线：性能指标 vs. 主要超参数 (例如 C)，用另一个超参数 (sigma) 分面或分组
# 假设我们主要看 Mean_F1
png(tuning_results_plot_file_svm, width = 1000, height = 700)
p_tune_svm <- ggplot(results_to_plot_svm, aes(x = C, y = Mean_F1, color = factor(sigma))) +
geom_line(linewidth = 1) +
geom_point(size = 3) +
geom_text(data = caret_svm_model$bestTune,
aes(label = paste("Best\nC:",C,"\nsigma:",sigma)),
vjust = -0.5, hjust = 0.5, color="black", size=3) +
scale_x_continuous(trans='log2', breaks=tune_grid_svm_radial$C) + # C通常在log尺度上调优
labs(title = paste("SVM (RBF Kernel) Tuning: Mean_F1 vs. C and sigma"),
subtitle = paste("Optimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma),
"\nAchieved Mean_F1 (CV) =", sprintf("%.4f", caret_svm_model$results[rownames(caret_svm_model$bestTune), "Mean_F1"])),
x = "C (Cost parameter, log2 scale)",
y = "Mean F1-Score (Cross-Validation)",
color = "Sigma (Kernel Width)") +
theme_minimal(base_size = 12) +
theme(legend.position = "top", plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
print(p_tune_svm)
# --- 6. 查看、记录和绘制调优结果 ---
cat("\n步骤4: 查看、记录和绘制SVM调优结果...\n")
cat("最优超参数组合 (Best Tune):\n"); print(caret_svm_model$bestTune)
cat("\n调优过程中的性能指标概览 (部分):\n"); print(head(caret_svm_model$results))
# 准备绘图数据
results_to_plot_svm <- caret_svm_model$results
# 【调试】检查 results_to_plot_svm$Mean_F1
cat("\n--- 调试: results_to_plot_svm$Mean_F1 (绘图前) ---\n")
print(head(results_to_plot_svm$Mean_F1))
cat("Is Mean_F1 column numeric? ", is.numeric(results_to_plot_svm$Mean_F1), "\n")
cat("Number of NAs in Mean_F1: ", sum(is.na(results_to_plot_svm$Mean_F1)), "\n")
cat("--- 调试结束 ---\n")
# 移除 Mean_F1 为 NA 的行，以避免 ggplot 问题 (虽然ggplot通常会处理，但明确些更好)
results_to_plot_svm_no_na <- results_to_plot_svm %>% filter(!is.na(Mean_F1))
if (nrow(results_to_plot_svm_no_na) == 0) {
cat("警告: 所有调优结果的 Mean_F1 都是NA，无法绘制调优曲线。\n")
} else {
# 获取最优模型的交叉验证性能 (更稳健的方式)
best_cv_perf <- getTrainPerf(caret_svm_model) # 包含TrainAccuracy, TrainKappa等 (基于最优模型在CV上的表现)
# best_cv_perf 列名可能包含 "Train" 前缀，我们需要的是 CV 上的 Mean_F1
# 从 caret_svm_model$results 中找到与 bestTune 对应的 Mean_F1
best_tune_f1_value <- NA
# 找到 bestTune 对应的行
# bestTune_params <- caret_svm_model$bestTune
# best_tune_row <- dplyr::inner_join(results_to_plot_svm, bestTune_params, by=names(bestTune_params))
# if(nrow(best_tune_row) == 1 && "Mean_F1" %in% names(best_tune_row)){
#    best_tune_f1_value <- best_tune_row$Mean_F1
# }
# 更简单：直接用 caret_svm_model$results[rownames(caret_svm_model$bestTune), ]
# 但要确保 rownames 匹配
best_tune_metric_values <- caret_svm_model$results[rownames(caret_svm_model$bestTune), ]
if("Mean_F1" %in% colnames(best_tune_metric_values)){
best_tune_f1_value <- best_tune_metric_values[["Mean_F1"]]
}
png(tuning_results_plot_file_svm, width = 1000, height = 700)
p_tune_svm <- ggplot(results_to_plot_svm_no_na, aes(x = C, y = Mean_F1, color = factor(sigma))) +
geom_line(linewidth = 1) +
geom_point(size = 3) +
# geom_text 的 data 参数应该包含 C 和 sigma 列，caret_svm_model$bestTune 满足这个条件
geom_text(data = caret_svm_model$bestTune,
aes(label = paste("Best\nC:",C,"\nsigma:",sigma)), # 这里 C 和 sigma 来自 caret_svm_model$bestTune
vjust = -0.8, hjust = 0.5, color="black", size=3.5, check_overlap = TRUE) +
scale_x_continuous(trans='log2', breaks=tune_grid_svm_radial$C) +
labs(title = paste("SVM (RBF Kernel) Tuning: Mean_F1 vs. C and sigma"),
subtitle = if(!is.na(best_tune_f1_value)) {
paste("Optimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma),
"\nAchieved Mean_F1 (CV) =", sprintf("%.4f", best_tune_f1_value))
} else {
paste("Optimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma),
"\nMean_F1 for best tune is NA or not found.")
},
x = "C (Cost parameter, log2 scale)",
y = "Mean F1-Score (Cross-Validation)",
color = "Sigma (Kernel Width)") +
theme_minimal(base_size = 12) +
theme(legend.position = "top", plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
print(p_tune_svm)
dev.off()
cat("SVM调优性能曲线图已保存到: '", tuning_results_plot_file_svm, "'\n")
}
# 将完整的调优结果和最优参数写入文件
sink(performance_metrics_file_svm, append = FALSE) # 新建或覆盖文件
# ... (后续写入文件的代码不变，但要确保 best_tune_results_svm 的获取是稳健的) ...
cat("--- Caret SVM Model Tuning Results ---\n")
cat("Method used: svmRadial (RBF Kernel)\n")
cat("Metric for optimization: ", caret_svm_model$metric, "\n")
cat("\nOptimal Hyperparameters (bestTune):\n"); print(caret_svm_model$bestTune)
cat("\nPerformance for Optimal Hyperparameters (from CV results):\n")
# 移除 Mean_F1 为 NA 的行，以避免 ggplot 问题 (虽然ggplot通常会处理，但明确些更好)
results_to_plot_svm_no_na <- results_to_plot_svm %>% filter(!is.na(Mean_F1))
if (nrow(results_to_plot_svm_no_na) == 0) {
cat("警告: 所有调优结果的 Mean_F1 都是NA，无法绘制调优曲线。\n")
} else {
# 获取最优模型的交叉验证性能中实际的 Mean_F1 值
# best_tune_metric_values 已经是 bestTune 那一行的数据了
best_tune_actual_C <- caret_svm_model$bestTune$C
best_tune_actual_sigma <- caret_svm_model$bestTune$sigma
# 从 results_to_plot_svm_no_na (或原始的 results_to_plot_svm) 中找到与 bestTune 匹配的行，以获取该点的 Mean_F1
# 这是为了确保 geom_text 的 y aesthetic 使用的是实际绘制数据中的值
best_tune_point_data <- results_to_plot_svm_no_na %>%
filter(C == best_tune_actual_C & sigma == best_tune_actual_sigma) %>%
head(1) # 应该只有一行匹配
# 准备副标题中要显示的 Mean_F1 值
best_tune_f1_for_subtitle <- NA
if (nrow(best_tune_point_data) == 1 && "Mean_F1" %in% names(best_tune_point_data)) {
best_tune_f1_for_subtitle <- best_tune_point_data$Mean_F1
} else {
# 如果在 no_na 版本中找不到（理论上不应发生如果 bestTune 的 F1 不是 NA），
# 尝试从原始 results 中找，以防 bestTune 的 F1 恰好是 NA
original_best_tune_row <- caret_svm_model$results[rownames(caret_svm_model$bestTune), ]
if ("Mean_F1" %in% names(original_best_tune_row)) {
best_tune_f1_for_subtitle <- original_best_tune_row[["Mean_F1"]]
}
}
png(tuning_results_plot_file_svm, width = 1000, height = 700)
p_tune_svm <- ggplot(results_to_plot_svm_no_na, aes(x = C, y = Mean_F1, color = factor(sigma))) +
geom_line(linewidth = 1) +
geom_point(size = 3) +
# 【修改 geom_text】
# data 参数仍然是 caret_svm_model$bestTune (因为它包含 C 和 sigma 用于标签文本)
# 但是 x 和 y aesthetic 需要明确指定，并且它们的值要对应于最佳点
# 如果 best_tune_point_data 找到了，用它的值；否则，geom_text 可能不显示或出错
{ if (nrow(best_tune_point_data) == 1) { # 确保我们找到了最佳点的数据
geom_text(data = best_tune_point_data, # 使用包含Mean_F1的这一行数据
aes(x = C, y = Mean_F1, # 明确指定 x 和 y aesthetic
label = paste("Best\nC:", sprintf("%.2f", C), "\nsigma:", sprintf("%.4f", sigma))),
vjust = -0.8, hjust = 0.5, color="black", size=3.5, check_overlap = TRUE,
inherit.aes = FALSE) # 【重要】阻止从顶层ggplot继承y=Mean_F1 (因为data不同)
# 实际上，由于我们显式提供了x和y，inherit.aes=FALSE可能不是必需的，
# 但它可以防止意外的继承。
# 或者，可以确保best_tune_point_data也有C和sigma列
} else {
# 如果找不到最佳点（例如，如果bestTune的Mean_F1是NA而被过滤掉了）
# 可以选择不添加这个geom_text，或者用caret_svm_model$bestTune来定位，但y值会缺失
geom_blank() # 或者不添加任何东西
cat("警告: 未能在绘图数据中定位最佳参数点以添加文本标签。\n")
}} +
scale_x_continuous(trans='log2', breaks=tune_grid_svm_radial$C) +
labs(title = paste("SVM (RBF Kernel) Tuning: Mean_F1 vs. C and sigma"),
subtitle = if(!is.na(best_tune_f1_for_subtitle)) {
paste("Optimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma),
"\nAchieved Mean_F1 (CV) =", sprintf("%.4f", best_tune_f1_for_subtitle))
} else {
paste("Optimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma),
"\nMean_F1 for best tune is NA or was not found in plotted data.")
},
x = "C (Cost parameter, log2 scale)",
y = "Mean F1-Score (Cross-Validation)",
color = "Sigma (Kernel Width)") +
theme_minimal(base_size = 12) +
theme(legend.position = "top", plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
print(p_tune_svm)
dev.off()
cat("SVM调优性能曲线图已保存到: '", tuning_results_plot_file_svm, "'\n")
}
# 将完整的调优结果和最优参数写入文件
sink(performance_metrics_file_svm, append = FALSE) # 新建或覆盖文件
# ... (后续写入文件的代码不变，但要确保 best_tune_results_svm 的获取是稳健的) ...
cat("--- Caret SVM Model Tuning Results ---\n")
cat("Method used: svmRadial (RBF Kernel)\n")
cat("Metric for optimization: ", caret_svm_model$metric, "\n")
cat("\nOptimal Hyperparameters (bestTune):\n"); print(caret_svm_model$bestTune)
cat("\nPerformance for Optimal Hyperparameters (from CV results):\n")
# best_tune_results_svm <- caret_svm_model$results[rownames(caret_svm_model$bestTune), ] # 这行可能因行名不匹配而出错
# 更稳健的获取方式：
best_tune_results_svm <- caret_svm_model$results %>%
dplyr::filter(C == caret_svm_model$bestTune$C & sigma == caret_svm_model$bestTune$sigma) %>%
head(1) # 确保只取一行，以防万一有浮点数匹配问题（理论上不应有）
if (nrow(best_tune_results_svm) == 1) {
print(best_tune_results_svm)
} else {
cat("警告: 未能唯一确定最优参数在results表中的行。打印getTrainPerf()的结果。\n")
print(getTrainPerf(caret_svm_model))
}
cat("\nFull Tuning Grid Results:\n"); print(caret_svm_model$results)
sink()
cat("SVM调优结果已保存到: '", performance_metrics_file_svm, "'\n")
# --- 7. 在测试集上评估【最优】SVM模型 (输出更全面的指标) ---
cat("\n步骤5: 在测试集上评估最优SVM模型 (更全面)...\n")
if (nrow(test_set) > 0 && ncol(test_set) > 1) {
predictions_test_class_tuned_svm <- predict(caret_svm_model, newdata = test_set)
test_labels_for_eval_cleaned <- test_set$label # 已经是清理后的level
if(length(predictions_test_class_tuned_svm) == length(test_labels_for_eval_cleaned) &&
length(predictions_test_class_tuned_svm) > 0) {
# 确保预测结果和真实标签的因子水平在传递给confusionMatrix前是完全一致的
# (predict.train 应该已经处理了，但双重检查无害)
predictions_test_class_tuned_svm <- factor(predictions_test_class_tuned_svm, levels = common_levels_cleaned)
test_labels_for_eval_cleaned <- factor(test_labels_for_eval_cleaned, levels = common_levels_cleaned)
conf_matrix_test_tuned_svm <- confusionMatrix(
data = predictions_test_class_tuned_svm,
reference = test_labels_for_eval_cleaned,
mode = "everything" # 获取所有指标: Sensitivity, Specificity, PPV, NPV, F1, Balanced Accuracy等
)
cat("\n混淆矩阵 (测试集 - 最优调优SVM模型 - 使用清理后标签名计算):\n")
print(conf_matrix_test_tuned_svm$table)
# 将结果中的类别名映射回原始可读名称用于报告
# caret的confusionMatrix的$byClass的行名是清理后的level
stats_by_class_svm_cleaned <- conf_matrix_test_tuned_svm$byClass
stats_by_class_svm_original_names <- stats_by_class_svm_cleaned
rownames(stats_by_class_svm_original_names) <- level_map_to_original[rownames(stats_by_class_svm_cleaned)]
# 对于混淆矩阵的维度名称
cm_table_original_names <- conf_matrix_test_tuned_svm$table
dimnames(cm_table_original_names) <- list(Prediction=level_map_to_original[rownames(cm_table_original_names)],
Reference=level_map_to_original[colnames(cm_table_original_names)])
sink(performance_metrics_file_svm, append = TRUE)
cat("\n\n--- Final SVM Model Performance on Test Set (Tuned SVM) ---\n")
cat("Optimal Hyperparameters used:\n"); print(caret_svm_model$bestTune)
cat("Features used in model: ", num_features_for_svm, "\n")
cat("亚型类别 (原始可读): ", paste(original_levels, collapse=", "), "\n\n")
cat("Confusion Matrix (Test Set - Original Readable Names):\n")
print(cm_table_original_names)
cat("\nOverall Statistics (Test Set):\n")
print(conf_matrix_test_tuned_svm$overall)
cat("\nStatistics by Class (Test Set - Original Readable Names):\n")
print(stats_by_class_svm_original_names)
# 手动计算并添加宏平均和加权平均指标
precision_by_class_svm <- stats_by_class_svm_cleaned[, "Pos Pred Value"]
recall_by_class_svm    <- stats_by_class_svm_cleaned[, "Sensitivity"]
f1_by_class_svm        <- stats_by_class_svm_cleaned[, "F1"]
# 移除NA值以正确计算平均值 (如果某些类别在测试集中没有样本或没有被预测)
precision_by_class_svm[is.na(precision_by_class_svm)] <- 0
recall_by_class_svm[is.na(recall_by_class_svm)]       <- 0
f1_by_class_svm[is.na(f1_by_class_svm)]               <- 0
macro_precision_svm <- mean(precision_by_class_svm)
macro_recall_svm    <- mean(recall_by_class_svm)
macro_f1_svm        <- mean(f1_by_class_svm)
# 支持度 (真实样本数) 用于加权平均
support_by_class_svm <- colSums(conf_matrix_test_tuned_svm$table) # 按真实类别
weighted_precision_svm <- weighted.mean(precision_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
weighted_recall_svm    <- weighted.mean(recall_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
weighted_f1_svm        <- weighted.mean(f1_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
cat("\n--- Averaged Metrics (Test Set - SVM) ---\n")
cat("Macro-Averaged Precision: ", sprintf("%.4f", macro_precision_svm), "\n")
cat("Macro-Averaged Recall:    ", sprintf("%.4f", macro_recall_svm), "\n")
cat("Macro-Averaged F1-Score:  ", sprintf("%.4f", macro_f1_svm), "\n")
cat("Weighted-Averaged Precision: ", sprintf("%.4f", weighted_precision_svm), "\n")
cat("Weighted-Averaged Recall:    ", sprintf("%.4f", weighted_recall_svm), "\n")
cat("Weighted-Averaged F1-Score:  ", sprintf("%.4f", weighted_f1_svm), "\n")
sink()
cat("SVM测试集评估结果已追加保存到: '", performance_metrics_file_svm, "'\n")
} else { cat("警告: SVM测试集预测或标签数据不足、不一致或为空，无法生成混淆矩阵。\n") }
} else { cat("测试集为空或无特征，跳过SVM最终模型评估。\n")}
if(exists("predictions_test_class_tuned_svm")) rm(predictions_test_class_tuned_svm)
if(exists("conf_matrix_test_tuned_svm")) rm(conf_matrix_test_tuned_svm); gc()
# --- 8. SVM特征重要性 (如果可用) ---
cat("\n步骤6: 提取最终SVM模型的特征重要性 (如果可用)...\n")
# ... (与之前SVM版本的特征重要性提取和绘图逻辑相同) ...
importance_obj_svm <- NULL; tryCatch({ importance_obj_svm <- varImp(caret_svm_model, scale = FALSE) }, error = function(e) { cat("提取SVM varImp失败:",conditionMessage(e),"\n")})
if (!is.null(importance_obj_svm) && !is.null(importance_obj_svm$importance) && nrow(importance_obj_svm$importance) > 0) {
imp_df_svm <- importance_obj_svm$importance
imp_scores_svm <- if("Overall" %in% colnames(imp_df_svm)) imp_df_svm[,"Overall", drop=FALSE] else imp_df_svm[,1, drop=FALSE]
importance_df_svm_sorted <- data.frame(Feature = rownames(imp_scores_svm), Importance = imp_scores_svm[[1]]) %>% arrange(desc(Importance))
cat("SVM特征重要性 (Top 20):\n"); print(head(importance_df_svm_sorted, 20))
png(importance_plot_file_svm, width = 1000, height = 700)
p_imp_svm <- ggplot(head(importance_df_svm_sorted, 20), aes(x = reorder(Feature, Importance), y = Importance)) + geom_bar(stat="identity", fill="steelblue") + coord_flip() + labs(title="Top 20 Important Features (Tuned SVM)", x="Feature", y="Importance Score") + theme_minimal(base_size = 10) + theme(plot.title = element_text(hjust = 0.5)); print(p_imp_svm); dev.off()
cat("SVM特征重要性图已保存到: '", importance_plot_file_svm, "'\n")
} else { cat("警告: 未能从调优的SVM模型中提取到有效的特征重要性数据。\n") }
if(exists("importance_obj_svm")) rm(importance_obj_svm); if(exists("importance_df_svm_sorted")) rm(importance_df_svm_sorted); gc()
# --- 9. 保存训练好的【最优】caret SVM模型 ---
cat("\n步骤7: 保存训练好的最优caret SVM模型...\n")
saveRDS(caret_svm_model, file = caret_model_output_file)
cat("最优caret SVM模型已保存到: '", caret_model_output_file, "'\n")
if(exists("caret_svm_model")) rm(caret_svm_model); gc()
cat("\n--- 模型训练与评估 (使用SVM通过caret进行调优) 完成！ ---\n")
# --- 6. 查看、记录和绘制调优结果 ---
cat("\n步骤4: 查看、记录和绘制SVM调优结果...\n")
cat("最优超参数组合 (Best Tune):\n"); print(caret_svm_model$bestTune)
cat("\n调优过程中的性能指标概览 (部分，基于最优 metric='", caret_svm_model$metric, "'):\n")
# --- 加载必要的R包 ---
suppressPackageStartupMessages(library(caret))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(MLmetrics)) # 用于手动计算某些指标 (如果需要)
suppressPackageStartupMessages(library(kernlab))   # svmRadial等可能依赖
suppressPackageStartupMessages(library(e1071))     # svmLinear等可能依赖
suppressPackageStartupMessages(library(reshape2))   # 用于melt数据，方便ggplot
# --- 参数定义与文件路径 ---
data_splits_dir <- "data_splits"
train_file <- file.path(data_splits_dir, "train_set_subtypes.rds")
validation_file <- file.path(data_splits_dir, "validation_set_subtypes.rds")
test_file <- file.path(data_splits_dir, "test_set_subtypes.rds")
model_output_dir <- "model"
caret_model_output_file <- file.path(model_output_dir, "caret_svm_model_tuned_subtypes_fullEval.rds")
results_output_dir <- "results"
tuning_results_plot_file_svm <- file.path(results_output_dir, "svm_tuning_metrics_plot.png") # 图名统一
importance_plot_file_svm <- file.path(results_output_dir, "svm_feat_imp_tuned_subtypes_fullEval.png")
performance_metrics_file_svm <- file.path(results_output_dir, "svm_perf_metrics_tuned_subtypes_fullEval.txt")
cat("--- 开始模型训练与评估 (SVM with caret, 增强评估和绘图) ---\n")
# --- 1. 创建输出目录 ---
if (!dir.exists(model_output_dir)) dir.create(model_output_dir, recursive = TRUE)
if (!dir.exists(results_output_dir)) dir.create(results_output_dir, recursive = TRUE)
# --- 2. 加载数据集与标签处理 ---
cat("步骤1: 加载数据集并处理标签...\n")
if (!all(file.exists(train_file, validation_file, test_file))) stop("错误: 一个或多个数据集文件未找到.")
train_set <- readRDS(train_file)
validation_set <- readRDS(validation_file)
test_set <- readRDS(test_file)
cat("亚型数据集加载完成:\n  训练集维度: ", paste(dim(train_set), collapse = " x "), "\n")
original_levels <- levels(train_set$label)
if (is.null(original_levels) || length(original_levels) < 2) stop("错误: 训练集标签问题.")
cleaned_levels <- make.names(original_levels, unique = TRUE)
level_map_to_original <- setNames(original_levels, cleaned_levels)
level_map_to_cleaned <- setNames(cleaned_levels, original_levels)
levels(train_set$label) <- cleaned_levels
levels(validation_set$label) <- cleaned_levels
levels(test_set$label) <- cleaned_levels
common_levels_cleaned <- cleaned_levels # caret将使用这些清理后的level
num_classes <- length(common_levels_cleaned)
cat("所有数据集的 'label' 列的因子水平已规范化为: ", paste(common_levels_cleaned, collapse=", "), "\n")
# --- 3. (可选) 初步特征选择 ---
num_features_for_svm <- ncol(train_set) - 1
cat("最终用于SVM训练的特征数量: ", num_features_for_svm, "\n")
if (num_features_for_svm > 1000) {
cat("警告: 特征数量 (", num_features_for_svm, ") 对于SVM仍然较多，训练可能非常慢。\n")
}
# --- 4. 设置caret训练控制参数 (SVM) ---
cat("\n步骤2: 设置caret训练控制参数 (SVM)...\n")
train_control_svm <- trainControl(
method = "cv", number = 5,
summaryFunction = multiClassSummary, # 获取 Accuracy, Kappa, Mean_F1, etc.
classProbs = TRUE,                   # 重要: 允许计算类别概率 (某些SVM方法支持)
savePredictions = "final",
verboseIter = TRUE,
allowParallel = TRUE                 # 如果设置了并行后端
)
# 定义SVM (RBF核) 超参数调优网格
# 【可调参数网格】根据数据和计算资源调整
# 如果tuneLength被指定，caret会自己生成tuneGrid中的参数值
# 我们手动指定，以更好地控制
tune_grid_svm_radial <- expand.grid(
sigma = 10^seq(-3, -1, length.out = 3), # 例如: 0.001, 0.01, 0.1
C = 2^seq(0, 4, length.out = 3)        # 例如: 1, 4, 16
)
cat("将使用以下参数组合数量进行SVM (RBF核) 调优: ", nrow(tune_grid_svm_radial), "\n")
print(tune_grid_svm_radial)
# --- 5. 使用caret训练SVM模型 ---
cat("\n步骤3: 使用caret训练SVM模型 (含超参数调优)...\n")
set.seed(123)
min_class_count_train <- min(table(train_set$label))
if (min_class_count_train < train_control_svm$number && train_control_svm$method == "cv") {
cat("警告: 最小类别样本数 (", min_class_count_train, ") < CV折叠数 (", train_control_svm$number, ").\n")
}
caret_svm_model <- NULL
tryCatch({
caret_svm_model <- train(label ~ ., data = train_set,
method = "svmRadial",      # RBF核SVM (kernlab或e1071)
trControl = train_control_svm,
tuneGrid = tune_grid_svm_radial,
metric = "Mean_F1",        # 用Mean F1选择最优模型
preProcess = c("center", "scale"), # 标准化特征
na.action = na.omit
# prob.model = TRUE # 对于kernlab的ksvm，确保概率模型被训练 (如果classProbs=TRUE)
)
}, error = function(e) {
cat("错误: caret SVM模型训练失败: ", conditionMessage(e), "\n"); caret_svm_model <<- NULL
})
if (is.null(caret_svm_model)) stop("caret SVM模型未能成功训练。")
cat("caret SVM模型训练和调优完成。\n")
# --- 6. 查看、记录和绘制调优结果 ---
cat("\n步骤4: 查看、记录和绘制SVM调优结果...\n")
cat("最优超参数组合 (Best Tune):\n"); print(caret_svm_model$bestTune)
cat("\n调优过程中的性能指标概览 (部分，基于最优 metric='", caret_svm_model$metric, "'):\n")
# 打印与最优模型相关的结果行，以及整体结果的头部
best_tune_cv_results <- caret_svm_model$results[rownames(caret_svm_model$bestTune), ]
print(best_tune_cv_results)
cat("查看所有调优参数组合的结果 (前6行):\n")
print(head(caret_svm_model$results))
# 使用caret内置的plot函数绘制调优结果图
# 它会自动选择合适的绘图方式，通常是性能指标 vs. 超参数
png(tuning_results_plot_file_svm, width = 800, height = 600)
plot(caret_svm_model,
main = paste("SVM (RBF Kernel) Tuning Results\nOptimal C =", sprintf("%.2f", caret_svm_model$bestTune$C),
", Optimal sigma =", sprintf("%.4f", caret_svm_model$bestTune$sigma)),
xlab = "Hyperparameter Combinations (see console for details)") # xlab可以更具体
dev.off()
cat("SVM调优性能图已保存到: '", tuning_results_plot_file_svm, "'\n")
# 将完整的调优结果和最优参数写入文件
sink(performance_metrics_file_svm, append = FALSE) # 新建或覆盖文件
cat("--- Caret SVM Model Tuning Results ---\n")
cat("Method used: svmRadial (RBF Kernel)\n")
cat("Metric for optimization: ", caret_svm_model$metric, "\n") # 例如 "Mean_F1"
cat("\nOptimal Hyperparameters (bestTune):\n"); print(caret_svm_model$bestTune)
cat("\nPerformance for Optimal Hyperparameters (from CV results on training data):\n")
# 从 results 数据框中筛选出与 bestTune 完全匹配的行
# best_tune_results_svm <- caret_svm_model$results %>%
#                          dplyr::semi_join(caret_svm_model$bestTune, by = names(caret_svm_model$bestTune))
# 或者更简单，如果 rownames(bestTune) 可靠：
best_tune_results_svm <- caret_svm_model$results[rownames(caret_svm_model$bestTune), ]
if (nrow(best_tune_results_svm) == 1) {
print(best_tune_results_svm)
} else { # 如果上面方法不唯一或失败，用getTrainPerf
cat("警告: 未能唯一确定最优参数在results表中的行。打印getTrainPerf()的结果 (这可能只包含一个指标)。\n")
print(getTrainPerf(caret_svm_model))
}
cat("\nFull Tuning Grid Results (all hyperparameter combinations tried):\n"); print(caret_svm_model$results)
# --- 7. 在测试集上评估【最优】SVM模型 (输出更全面的指标) ---
cat("\n步骤5: 在测试集上评估最优SVM模型 (更全面)...\n")
if (nrow(test_set) > 0 && ncol(test_set) > 1) {
predictions_test_class_tuned_svm <- predict(caret_svm_model, newdata = test_set)
test_labels_for_eval_cleaned <- test_set$label # 已经是清理后的level
if(length(predictions_test_class_tuned_svm) == length(test_labels_for_eval_cleaned) &&
length(predictions_test_class_tuned_svm) > 0) {
predictions_test_class_tuned_svm <- factor(predictions_test_class_tuned_svm, levels = common_levels_cleaned)
test_labels_for_eval_cleaned <- factor(test_labels_for_eval_cleaned, levels = common_levels_cleaned)
conf_matrix_test_tuned_svm <- confusionMatrix(
data = predictions_test_class_tuned_svm,
reference = test_labels_for_eval_cleaned,
mode = "everything"
)
# cat("\n混淆矩阵 (测试集 - 最优调优SVM模型 - 使用清理后标签名计算):\n")
# print(conf_matrix_test_tuned_svm$table)
stats_by_class_svm_cleaned <- conf_matrix_test_tuned_svm$byClass
stats_by_class_svm_original_names <- stats_by_class_svm_cleaned
rownames(stats_by_class_svm_original_names) <- level_map_to_original[rownames(stats_by_class_svm_cleaned)]
cm_table_original_names <- conf_matrix_test_tuned_svm$table
dimnames(cm_table_original_names) <- list(Prediction=level_map_to_original[rownames(cm_table_original_names)],
Reference=level_map_to_original[colnames(cm_table_original_names)])
# --- 将所有评估结果写入文件 ---
# sink(performance_metrics_file_svm, append = TRUE) # 确保是追加模式
cat("\n\n--- Final SVM Model Performance on Test Set (Tuned SVM) ---\n")
cat("Optimal Hyperparameters used:\n"); print(caret_svm_model$bestTune)
cat("Number of features used in model: ", num_features_for_svm, "\n") # 使用之前保存的特征数
cat("Subtype categories (original readable names): ", paste(original_levels, collapse=", "), "\n\n")
cat("Confusion Matrix (Test Set - Original Readable Names):\n")
print(cm_table_original_names)
cat("\nOverall Statistics (Test Set):\n")
print(conf_matrix_test_tuned_svm$overall)
cat("\nStatistics by Class (Test Set - Original Readable Names):\n")
print(stats_by_class_svm_original_names)
# 手动计算并添加宏平均和加权平均指标
precision_by_class_svm <- stats_by_class_svm_cleaned[, "Pos Pred Value"]
recall_by_class_svm    <- stats_by_class_svm_cleaned[, "Sensitivity"]
f1_by_class_svm        <- stats_by_class_svm_cleaned[, "F1"]
precision_by_class_svm[is.na(precision_by_class_svm)] <- 0
recall_by_class_svm[is.na(recall_by_class_svm)]       <- 0
f1_by_class_svm[is.na(f1_by_class_svm)]               <- 0
macro_precision_svm <- mean(precision_by_class_svm)
macro_recall_svm    <- mean(recall_by_class_svm)
macro_f1_svm        <- mean(f1_by_class_svm)
support_by_class_svm <- colSums(conf_matrix_test_tuned_svm$table)
weighted_precision_svm <- weighted.mean(precision_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
weighted_recall_svm    <- weighted.mean(recall_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
weighted_f1_svm        <- weighted.mean(f1_by_class_svm, w = support_by_class_svm, na.rm = TRUE)
cat("\n--- Averaged Metrics (Test Set - SVM) ---\n")
cat("Macro-Averaged Precision: ", sprintf("%.4f", macro_precision_svm), "\n")
cat("Macro-Averaged Recall:    ", sprintf("%.4f", macro_recall_svm), "\n")
cat("Macro-Averaged F1-Score:  ", sprintf("%.4f", macro_f1_svm), "\n")
cat("Weighted-Averaged Precision: ", sprintf("%.4f", weighted_precision_svm), "\n")
cat("Weighted-Averaged Recall:    ", sprintf("%.4f", weighted_recall_svm), "\n")
cat("Weighted-Averaged F1-Score:  ", sprintf("%.4f", weighted_f1_svm), "\n")
sink() # 关闭 sink，所有内容写入文件
cat("SVM调优和测试集评估结果已保存到: '", performance_metrics_file_svm, "'\n")
} else {
cat("警告: SVM测试集预测或标签数据不足、不一致或为空，无法生成混淆矩阵。\n")
if(exists("performance_metrics_file_svm") && file.exists(performance_metrics_file_svm)) sink() # 确保关闭sink
}
} else {
cat("测试集为空或无特征，跳过SVM最终模型评估。\n")
if(exists("performance_metrics_file_svm") && file.exists(performance_metrics_file_svm)) sink() # 确保关闭sink
}
if(exists("predictions_test_class_tuned_svm")) rm(predictions_test_class_tuned_svm)
if(exists("conf_matrix_test_tuned_svm")) rm(conf_matrix_test_tuned_svm); gc()
# --- 8. SVM特征重要性 (如果可用) ---
cat("\n步骤6: 提取最终SVM模型的特征重要性 (如果可用)...\n")
# ... (与之前SVM版本的特征重要性提取和绘图逻辑相同) ...
importance_obj_svm <- NULL; tryCatch({ importance_obj_svm <- varImp(caret_svm_model, scale = FALSE) }, error = function(e) { cat("提取SVM varImp失败:",conditionMessage(e),"\n")})
if (!is.null(importance_obj_svm) && !is.null(importance_obj_svm$importance) && nrow(importance_obj_svm$importance) > 0) {
imp_df_svm <- importance_obj_svm$importance
imp_scores_svm <- if("Overall" %in% colnames(imp_df_svm)) imp_df_svm[,"Overall", drop=FALSE] else imp_df_svm[,1, drop=FALSE]
importance_df_svm_sorted <- data.frame(Feature = rownames(imp_scores_svm), Importance = imp_scores_svm[[1]]) %>% arrange(desc(Importance))
cat("SVM特征重要性 (Top 20):\n"); print(head(importance_df_svm_sorted, 20))
png(importance_plot_file_svm, width = 1000, height = 700)
p_imp_svm <- ggplot(head(importance_df_svm_sorted, 20), aes(x = reorder(Feature, Importance), y = Importance)) + geom_bar(stat="identity", fill="steelblue") + coord_flip() + labs(title="Top 20 Important Features (Tuned SVM)", x="Feature", y="Importance Score") + theme_minimal(base_size = 10) + theme(plot.title = element_text(hjust = 0.5)); print(p_imp_svm); dev.off()
cat("SVM特征重要性图已保存到: '", importance_plot_file_svm, "'\n")
} else { cat("警告: 未能从调优的SVM模型中提取到有效的特征重要性数据。\n") }
if(exists("importance_obj_svm")) rm(importance_obj_svm); if(exists("importance_df_svm_sorted")) rm(importance_df_svm_sorted); gc()
# --- 9. 保存训练好的【最优】caret SVM模型 ---
cat("\n步骤7: 保存训练好的最优caret SVM模型...\n")
saveRDS(caret_svm_model, file = caret_model_output_file)
cat("最优caret SVM模型已保存到: '", caret_model_output_file, "'\n")
if(exists("caret_svm_model")) rm(caret_svm_model); gc()
cat("\n--- 模型训练与评估 (使用SVM通过caret进行调优) 完成！ ---\n")
gc()
gc()
