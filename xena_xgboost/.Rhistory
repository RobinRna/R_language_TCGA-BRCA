# --- 8. 在测试集上评估模型性能 ---
cat("\n步骤6: 在测试集上评估XGBoost模型...\n")
if (nrow(test_set_df) > 0 && ncol(test_set_df) > 1) {
# 预测概率 (如果 objective = "multi:softprob")
pred_probs_xgb_test <- predict(final_xgb_model, dtest, reshape = TRUE) # reshape=TRUE使输出为 N x num_class 矩阵
# 将概率转换为类别预测 (选择概率最高的类别)
pred_labels_numeric_xgb_test <- max.col(pred_probs_xgb_test) - 1 # max.col返回1-based, 我们需要0-based
# 将数值标签转换回原始因子水平，以便confusionMatrix正确显示类别名
pred_labels_factor_xgb_test <- numeric_map_to_original_factor[as.character(pred_labels_numeric_xgb_test)]
test_labels_factor_original <- numeric_map_to_original_factor[as.character(test_labels_numeric)]
# 确保因子水平一致
pred_labels_factor_xgb_test <- factor(pred_labels_factor_xgb_test, levels = original_levels)
test_labels_factor_original <- factor(test_labels_factor_original, levels = original_levels)
if(length(pred_labels_factor_xgb_test) == length(test_labels_factor_original) && length(pred_labels_factor_xgb_test) > 0) {
conf_matrix_test_xgb <- confusionMatrix(pred_labels_factor_xgb_test, test_labels_factor_original)
cat("\n混淆矩阵 (测试集 - XGBoost模型):\n")
print(conf_matrix_test_xgb)
sink(performance_metrics_file)
cat("--- XGBoost Model Performance on Test Set (Direct Usage) ---\n\n")
cat("Model Parameters:\n"); print(params)
cat("\nNrounds used: ", final_xgb_model$best_iteration %||% best_nrounds, "\n") # final_xgb_model$best_iteration 如果有早停
cat("Features used in model: ", final_num_features, "\n")
cat("亚型类别 (原始): ", paste(original_levels, collapse=", "), "\n\n")
print(conf_matrix_test_xgb)
cat("\n--- 整体统计 (测试集 - XGBoost) ---\n")
cat("准确率 (Overall Accuracy): ", sprintf("%.4f", conf_matrix_test_xgb$overall['Accuracy']), "\n")
cat("Kappa:                    ", sprintf("%.4f", conf_matrix_test_xgb$overall['Kappa']), "\n")
# 可以计算其他指标如 F1-score by class, etc.
# f1_scores_by_class <- MLmetrics::F1_Score(y_true = test_labels_factor_original,
#                                           y_pred = pred_labels_factor_xgb_test,
#                                           positive = NULL) # positive=NULL for macro/micro average or per class
# cat("\nF1 Scores (per class or averaged - check MLmetrics doc):\n")
# print(f1_scores_by_class)
sink()
cat("XGBoost测试集评估结果已保存到: '", performance_metrics_file, "'\n")
} else { cat("警告: XGBoost测试集预测或标签数据不一致或为空。\n") }
} else { cat("测试集为空或无特征，跳过XGBoost最终模型评估。\n")}
# --- 9. XGBoost特征重要性 ---
cat("\n步骤7: 提取、排序并可视化XGBoost模型的特征重要性...\n")
tryCatch({
importance_matrix_xgb <- xgb.importance(feature_names = colnames(train_features_matrix), model = final_xgb_model)
if (!is.null(importance_matrix_xgb) && nrow(importance_matrix_xgb) > 0) {
cat("XGBoost特征重要性 (Top 20):\n")
print(head(importance_matrix_xgb, 20))
png(importance_plot_file, width = 1000, height = 700)
xgb.plot.importance(importance_matrix_xgb, top_n = min(20, nrow(importance_matrix_xgb)),
main = "Top Important Features (XGBoost)")
dev.off()
cat("XGBoost特征重要性图已保存到: '", importance_plot_file, "'\n")
} else {
cat("警告: 未能从XGBoost模型中提取特征重要性。\n")
}
}, error = function(e) {
cat("错误: 提取XGBoost特征重要性时失败: ", conditionMessage(e), "\n")
})
# --- 10. 保存训练好的XGBoost模型 ---
cat("\n步骤8: 保存训练好的XGBoost模型...\n")
xgb.save(final_xgb_model, fname = xgb_model_output_file)
# 或者用R的saveRDS
# saveRDS(final_xgb_model, file = xgb_model_output_file)
cat("XGBoost模型已保存到: '", xgb_model_output_file, "'\n")
cat("\n--- 模型训练与评估 (直接使用xgboost包) 完成！ ---\n")
# --- 加载必要的R包 ---
suppressPackageStartupMessages(library(xgboost))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(Matrix))
suppressPackageStartupMessages(library(MLmetrics)) # 用于F1, Precision, Recall等
suppressPackageStartupMessages(library(caret))     # 仍然使用 confusionMatrix
suppressPackageStartupMessages(library(reshape2)) # 用于melt数据，方便ggplot绘图
# --- 参数定义与文件路径 ---
data_splits_dir <- "data_splits"
# 【确保这些文件名与03脚本的输出一致】
train_file <- file.path(data_splits_dir, "train_set_subtypes.rds")
validation_file <- file.path(data_splits_dir, "validation_set_subtypes.rds")
test_file <- file.path(data_splits_dir, "test_set_subtypes.rds")
model_output_dir <- "model"
xgb_model_output_file <- file.path(model_output_dir, "xgboost_model_direct_subtypes_fullEval.rds")
results_output_dir <- "results"
cv_metrics_plot_file <- file.path(results_output_dir, "xgb_cv_metrics_plot.png")
train_metrics_plot_file <- file.path(results_output_dir, "xgb_train_metrics_plot.png")
importance_plot_file <- file.path(results_output_dir, "xgb_direct_feat_imp_subtypes_fullEval.png")
performance_metrics_file <- file.path(results_output_dir, "xgb_direct_perf_metrics_subtypes_fullEval.txt")
cat("--- 开始模型训练与评估 (直接使用xgboost, 增强评估) ---\n")
# --- 1. 创建输出目录 ---
if (!dir.exists(model_output_dir)) dir.create(model_output_dir, recursive = TRUE)
if (!dir.exists(results_output_dir)) dir.create(results_output_dir, recursive = TRUE)
# --- 2. 加载数据集 ---
cat("步骤1: 加载亚型训练集、验证集和测试集...\n")
if (!all(file.exists(train_file, validation_file, test_file))) stop("错误: 一个或多个数据集文件未找到。")
train_set_df <- readRDS(train_file)
validation_set_df <- readRDS(validation_file)
test_set_df <- readRDS(test_file)
cat("亚型数据集加载完成:\n")
cat("  训练集维度: ", paste(dim(train_set_df), collapse = " x "), "\n") # 特征数应已由02脚本控制
# --- 2.1 规范化类别标签并转换为数值 (0 to num_class-1) ---
cat("步骤1.1: 规范化类别标签并转换为数值...\n")
original_levels <- levels(train_set_df$label)
if (is.null(original_levels) || length(original_levels) < 2) {
stop("错误: 训练集的亚型标签列不是有效的因子或亚型类别少于2。")
}
# (映射表逻辑同前一个版本)
level_map_to_numeric <- setNames(0:(length(original_levels)-1), original_levels)
numeric_map_to_original_factor <- setNames(factor(original_levels, levels=original_levels), 0:(length(original_levels)-1))
train_labels_numeric <- level_map_to_numeric[as.character(train_set_df$label)]
validation_labels_numeric <- level_map_to_numeric[as.character(validation_set_df$label)]
test_labels_numeric <- level_map_to_numeric[as.character(test_set_df$label)]
num_classes <- length(original_levels)
cat("类别数量: ", num_classes, "\n")
# --- 3. 【移除】初步特征选择步骤 ---
# 假设02_data_preprocessing.R中的高方差预过滤已将特征数控制在可接受范围
# (例如，之前设定的TARGET_NUM_FEATURES_PREFILTER = 2000)
cat("\n步骤1.5: (跳过初步RF特征选择，依赖02脚本的预过滤)\n")
final_num_features <- ncol(train_set_df) - 1
cat("最终用于XGBoost训练的特征数量: ", final_num_features, "\n")
if (final_num_features > 3000) { # 再次提醒
cat("警告: 特征数量 (", final_num_features, ") 仍然较多，XGBoost训练可能较慢。\n")
}
# --- 4. 准备XGBoost数据格式 (xgb.DMatrix) ---
cat("\n步骤2: 准备XGBoost数据格式 (xgb.DMatrix)...\n")
# (DMatrix创建逻辑同前)
train_features_matrix <- as.matrix(train_set_df[, -which(names(train_set_df) == "label")])
validation_features_matrix <- as.matrix(validation_set_df[, -which(names(validation_set_df) == "label")])
test_features_matrix <- as.matrix(test_set_df[, -which(names(test_set_df) == "label")])
if(!is.numeric(train_features_matrix)) storage.mode(train_features_matrix) <- "numeric"
if(!is.numeric(validation_features_matrix)) storage.mode(validation_features_matrix) <- "numeric"
if(!is.numeric(test_features_matrix)) storage.mode(test_features_matrix) <- "numeric"
dtrain <- xgb.DMatrix(data = train_features_matrix, label = train_labels_numeric)
dvalidation <- xgb.DMatrix(data = validation_features_matrix, label = validation_labels_numeric)
dtest <- xgb.DMatrix(data = test_features_matrix, label = test_labels_numeric)
# --- 5. 设置XGBoost参数 ---
cat("\n步骤3: 设置XGBoost参数...\n")
# 【可调参数组】这些参数是超参数调优的候选
# 为了演示，我们先用一组固定的值，实际中这些应该通过网格搜索或类似方法确定
params <- list(
objective = "multi:softprob",  # 输出每个类别的概率
eval_metric = "mlogloss",      # 多分类对数损失
eval_metric = "merror",        # 多分类错误率 (可以同时监控多个)
num_class = num_classes,
eta = 0.1,                     # 学习率
max_depth = 4,                 # 最大深度
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 1,
gamma = 0
# alpha = 0, lambda = 1 # L1, L2正则化 (可选)
)
# --- 6. 使用xgb.cv进行交叉验证确定nrounds ---
cat("\n步骤4: 使用xgb.cv进行交叉验证以辅助确定nrounds...\n")
set.seed(123)
NROUNDS_CV_MAX <- 300 # 增加CV轮数尝试
EARLY_STOPPING_ROUNDS_CV <- 30 # CV中的早停
xgb_cv_results <- NULL
tryCatch({
xgb_cv_results <- xgb.cv(
params = params,
data = dtrain,
nrounds = NROUNDS_CV_MAX,
nfold = 5,
showsd = TRUE,
stratified = TRUE,
print_every_n = 20,
early_stopping_rounds = EARLY_STOPPING_ROUNDS_CV,
maximize = FALSE, # 对于mlogloss和merror都是越小越好
# metrics = list("mlogloss", "merror") # 可以监控多个指标
)
}, error = function(e) {
cat("错误: xgb.cv 失败: ", conditionMessage(e), "\n")
xgb_cv_results <<- NULL
})
if (is.null(xgb_cv_results)) stop("xgb.cv 执行失败，无法继续。")
cat("xgb.cv完成。\n")
best_nrounds_mlogloss <- xgb_cv_results$best_iteration # 基于第一个eval_metric (mlogloss)
# 如果有多个eval_metric, xgb.cv$best_iteration 是基于第一个的
# 如果想基于merror:
# best_nrounds_merror <- xgb_cv_results$evaluation_log[which.min(test_merror_mean), iter]
# 我们这里统一使用mlogloss的早停结果
best_nrounds <- best_nrounds_mlogloss
cat("根据xgb.cv (mlogloss), 最优的nrounds约: ", best_nrounds, "\n")
if (is.null(best_nrounds) || best_nrounds < 20) {
cat("警告: xgb.cv 结果不理想或轮数过少。将使用一个备用nrounds值 (例如100)。\n")
best_nrounds <- 100
}
# 绘制CV结果 (mlogloss 和 merror vs nrounds)
cv_log_df <- as.data.frame(xgb_cv_results$evaluation_log)
cv_log_df_melt <- melt(cv_log_df, id.vars = "iter",
measure.vars = c("train_mlogloss_mean", "test_mlogloss_mean",
"train_merror_mean", "test_merror_mean"),
variable.name = "metric_type", value.name = "value")
png(cv_metrics_plot_file, width = 1200, height = 700)
p_cv <- ggplot(cv_log_df_melt, aes(x = iter, y = value, color = metric_type)) +
geom_line(linewidth = 1) +
geom_vline(xintercept = best_nrounds, linetype = "dashed", color = "red") +
labs(title = paste("XGBoost CV Metrics vs. Number of Rounds\nOptimal nrounds (mlogloss based) ~", best_nrounds),
x = "Number of Rounds (Iteration)", y = "Metric Value") +
scale_color_manual(values = c("train_mlogloss_mean" = "blue", "test_mlogloss_mean" = "cornflowerblue",
"train_merror_mean" = "red", "test_merror_mean" = "salmon"),
labels = c("Train mlogloss", "Test mlogloss", "Train merror", "Test merror")) +
theme_minimal() +
theme(legend.position = "top")
print(p_cv)
dev.off()
cat("XGBoost CV 指标曲线图已保存到: '", cv_metrics_plot_file, "'\n")
# --- 7. 训练最终XGBoost模型 ---
cat("\n步骤5: 训练最终XGBoost模型 (使用确定的nrounds)...\n")
set.seed(123)
watchlist <- list(train = dtrain, eval = dvalidation) # 监控训练集和验证集
final_xgb_model <- NULL
training_history <- NULL # 用于存储训练过程中的指标
tryCatch({
# 使用 xgb.train 并捕获评估日志
# verbose=1 会打印评估结果, verbose=0 静默
# 如果想手动捕获，可以使用 callbacks = list(cb.evaluation.log())
# 但更简单的是在watchlist中指定，然后从 $evaluation_log 提取
final_xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = best_nrounds,
watchlist = watchlist,
print_every_n = 10, # 每10轮打印一次
# early_stopping_rounds = if (best_nrounds > 50) EARLY_STOPPING_ROUNDS_CV else NULL, # 最终模型也可以用早停
verbose = 1
)
if (!is.null(final_xgb_model$evaluation_log)) {
training_history <- as.data.frame(final_xgb_model$evaluation_log)
}
}, error = function(e) {
cat("错误: 训练最终XGBoost模型时失败: ", conditionMessage(e), "\n")
final_xgb_model <<- NULL
})
if(is.null(final_xgb_model)) stop("最终XGBoost模型训练失败。")
cat("最终XGBoost模型训练完成。\n")
# 绘制训练过程中的指标曲线 (类似loss/accuracy曲线)
if (!is.null(training_history) && nrow(training_history) > 0) {
train_hist_melt <- melt(training_history, id.vars = "iter",
measure.vars = grep("^(train|eval)_(mlogloss|merror)$", names(training_history), value = TRUE),
variable.name = "metric_type", value.name = "value")
png(train_metrics_plot_file, width = 1200, height = 700)
p_train_hist <- ggplot(train_hist_melt, aes(x = iter, y = value, color = metric_type)) +
geom_line(linewidth = 1) +
labs(title = "XGBoost Training History (Metrics vs. Rounds)",
x = "Number of Rounds (Iteration)", y = "Metric Value") +
scale_color_manual(values = c("train_mlogloss" = "darkblue", "eval_mlogloss" = "blue",
"train_merror" = "darkred", "eval_merror" = "red"),
labels = c("Train mlogloss", "Validation mlogloss", "Train merror", "Validation merror")) +
theme_minimal() +
theme(legend.position = "top")
print(p_train_hist)
dev.off()
cat("XGBoost训练历史指标曲线图已保存到: '", train_metrics_plot_file, "'\n")
} else {
cat("警告: 未能获取XGBoost训练历史用于绘图。\n")
}
# --- 8. 在测试集上评估模型性能 (更全面) ---
cat("\n步骤6: 在测试集上评估XGBoost模型 (更全面)...\n")
# ... (预测和混淆矩阵部分同前) ...
if (nrow(test_set_df) > 0 && ncol(test_set_df) > 1) {
pred_probs_xgb_test <- predict(final_xgb_model, dtest, reshape = TRUE)
pred_labels_numeric_xgb_test <- max.col(pred_probs_xgb_test) - 1
pred_labels_factor_xgb_test <- numeric_map_to_original_factor[as.character(pred_labels_numeric_xgb_test)]
test_labels_factor_original <- numeric_map_to_original_factor[as.character(test_labels_numeric)]
pred_labels_factor_xgb_test <- factor(pred_labels_factor_xgb_test, levels = original_levels)
test_labels_factor_original <- factor(test_labels_factor_original, levels = original_levels)
if(length(pred_labels_factor_xgb_test) == length(test_labels_factor_original) && length(pred_labels_factor_xgb_test) > 0) {
conf_matrix_test_xgb <- confusionMatrix(pred_labels_factor_xgb_test, test_labels_factor_original)
cat("\n混淆矩阵 (测试集 - XGBoost模型):\n")
print(conf_matrix_test_xgb)
# 计算每个类别的 Precision, Recall, F1-score
precision_by_class <- diag(conf_matrix_test_xgb$table) / colSums(conf_matrix_test_xgb$table)
recall_by_class <- diag(conf_matrix_test_xgb$table) / rowSums(conf_matrix_test_xgb$table)
f1_by_class <- 2 * (precision_by_class * recall_by_class) / (precision_by_class + recall_by_class)
f1_by_class[is.na(f1_by_class)] <- 0 # 处理分母为0的情况
# Macro averages (简单平均)
macro_precision <- mean(precision_by_class, na.rm = TRUE)
macro_recall <- mean(recall_by_class, na.rm = TRUE)
macro_f1 <- mean(f1_by_class, na.rm = TRUE)
# Weighted averages (按支持度加权)
support_by_class <- rowSums(conf_matrix_test_xgb$table)
weighted_precision <- weighted.mean(precision_by_class, w = support_by_class, na.rm = TRUE)
weighted_recall <- weighted.mean(recall_by_class, w = support_by_class, na.rm = TRUE)
weighted_f1 <- weighted.mean(f1_by_class, w = support_by_class, na.rm = TRUE)
sink(performance_metrics_file)
cat("--- XGBoost Model Performance on Test Set (Direct Usage, Full Eval) ---\n\n")
# ... (模型参数等信息同前) ...
print(conf_matrix_test_xgb) # 包含caret计算的byClass指标
cat("\n--- Manually Calculated Metrics (Test Set - XGBoost) ---\n")
cat("Precision by Class:\n"); print(precision_by_class)
cat("Recall by Class:\n"); print(recall_by_class)
cat("F1-Score by Class:\n"); print(f1_by_class)
cat("\nMacro-Averaged Precision: ", sprintf("%.4f", macro_precision), "\n")
cat("Macro-Averaged Recall:    ", sprintf("%.4f", macro_recall), "\n")
cat("Macro-Averaged F1-Score:  ", sprintf("%.4f", macro_f1), "\n")
cat("\nWeighted-Averaged Precision: ", sprintf("%.4f", weighted_precision), "\n")
cat("Weighted-Averaged Recall:    ", sprintf("%.4f", weighted_recall), "\n")
cat("Weighted-Averaged F1-Score:  ", sprintf("%.4f", weighted_f1), "\n")
cat("\n--- Caret Overall Statistics ---\n")
cat("Accuracy: ", sprintf("%.4f", conf_matrix_test_xgb$overall['Accuracy']), "\n")
cat("Kappa:    ", sprintf("%.4f", conf_matrix_test_xgb$overall['Kappa']), "\n")
sink()
cat("XGBoost测试集评估结果已保存到: '", performance_metrics_file, "'\n")
} else { cat("警告: XGBoost测试集预测或标签数据不一致或为空。\n") }
} else { cat("测试集为空或无特征，跳过XGBoost最终模型评估。\n")}
# --- 9. XGBoost特征重要性 --- (逻辑同前)
cat("\n步骤7: 提取、排序并可视化XGBoost模型的特征重要性...\n")
# ... (xgb.importance 和 xgb.plot.importance 逻辑不变) ...
tryCatch({
importance_matrix_xgb <- xgb.importance(feature_names = colnames(train_features_matrix), model = final_xgb_model)
if (!is.null(importance_matrix_xgb) && nrow(importance_matrix_xgb) > 0) {
cat("XGBoost特征重要性 (Top 20):\n"); print(head(importance_matrix_xgb, 20))
png(importance_plot_file, width = 1000, height = 700)
xgb.plot.importance(importance_matrix_xgb, top_n = min(20, nrow(importance_matrix_xgb)), main = "Top Important Features (XGBoost Direct)")
dev.off()
cat("XGBoost特征重要性图已保存到: '", importance_plot_file, "'\n")
} else { cat("警告: 未能从XGBoost模型中提取特征重要性。\n") }
}, error = function(e) { cat("错误: 提取XGBoost特征重要性时失败: ", conditionMessage(e), "\n") })
# --- 10. 保存训练好的XGBoost模型 ---
cat("\n步骤8: 保存训练好的XGBoost模型...\n")
# ... (xgb.save 或 saveRDS 逻辑不变) ...
xgb.save(final_xgb_model, fname = xgb_model_output_file)
cat("XGBoost模型已保存到: '", xgb_model_output_file, "'\n")
cat("\n--- 模型训练与评估 (直接使用xgboost, 增强评估) 完成！ ---\n")
# 【可调参数组】这些参数是超参数调优的候选
# 为了演示，我们先用一组固定的值，实际中这些应该通过网格搜索或类似方法确定
params <- list(
objective = "multi:softprob",  # 输出每个类别的概率
eval_metric = "mlogloss",      # 多分类对数损失
eval_metric = "merror",        # 多分类错误率 (可以同时监控多个)
num_class = num_classes,
eta = 0.1,                     # 学习率
max_depth = 4,                 # 最大深度
subsample = 0.8,
colsample_bytree = 0.8,
min_child_weight = 1,
gamma = 0
# alpha = 0, lambda = 1 # L1, L2正则化 (可选)
)
# --- 6. 使用xgb.cv进行交叉验证确定nrounds ---
cat("\n步骤4: 使用xgb.cv进行交叉验证以辅助确定nrounds...\n")
set.seed(123)
NROUNDS_CV_MAX <- 300 # 增加CV轮数尝试
EARLY_STOPPING_ROUNDS_CV <- 30 # CV中的早停
NROUNDS_CV_MAX <- 500 # 增加CV轮数尝试
EARLY_STOPPING_ROUNDS_CV <- 30 # CV中的早停
xgb_cv_results <- NULL
tryCatch({
xgb_cv_results <- xgb.cv(
params = params,
data = dtrain,
nrounds = NROUNDS_CV_MAX,
nfold = 5,
showsd = TRUE,
stratified = TRUE,
print_every_n = 20,
early_stopping_rounds = EARLY_STOPPING_ROUNDS_CV,
maximize = FALSE, # 对于mlogloss和merror都是越小越好
# metrics = list("mlogloss", "merror") # 可以监控多个指标
)
}, error = function(e) {
cat("错误: xgb.cv 失败: ", conditionMessage(e), "\n")
xgb_cv_results <<- NULL
})
if (is.null(xgb_cv_results)) stop("xgb.cv 执行失败，无法继续。")
cat("xgb.cv完成。\n")
best_nrounds_mlogloss <- xgb_cv_results$best_iteration # 基于第一个eval_metric (mlogloss)
# 如果有多个eval_metric, xgb.cv$best_iteration 是基于第一个的
# 如果想基于merror:
# best_nrounds_merror <- xgb_cv_results$evaluation_log[which.min(test_merror_mean), iter]
# 我们这里统一使用mlogloss的早停结果
best_nrounds <- best_nrounds_mlogloss
cat("根据xgb.cv (mlogloss), 最优的nrounds约: ", best_nrounds, "\n")
if (is.null(best_nrounds) || best_nrounds < 20) {
cat("警告: xgb.cv 结果不理想或轮数过少。将使用一个备用nrounds值 (例如100)。\n")
best_nrounds <- 100
}
# 绘制CV结果 (mlogloss 和 merror vs nrounds)
cv_log_df <- as.data.frame(xgb_cv_results$evaluation_log)
cv_log_df_melt <- melt(cv_log_df, id.vars = "iter",
measure.vars = c("train_mlogloss_mean", "test_mlogloss_mean",
"train_merror_mean", "test_merror_mean"),
variable.name = "metric_type", value.name = "value")
png(cv_metrics_plot_file, width = 1200, height = 700)
p_cv <- ggplot(cv_log_df_melt, aes(x = iter, y = value, color = metric_type)) +
geom_line(linewidth = 1) +
geom_vline(xintercept = best_nrounds, linetype = "dashed", color = "red") +
labs(title = paste("XGBoost CV Metrics vs. Number of Rounds\nOptimal nrounds (mlogloss based) ~", best_nrounds),
x = "Number of Rounds (Iteration)", y = "Metric Value") +
scale_color_manual(values = c("train_mlogloss_mean" = "blue", "test_mlogloss_mean" = "cornflowerblue",
"train_merror_mean" = "red", "test_merror_mean" = "salmon"),
labels = c("Train mlogloss", "Test mlogloss", "Train merror", "Test merror")) +
theme_minimal() +
theme(legend.position = "top")
print(p_cv)
dev.off()
cat("XGBoost CV 指标曲线图已保存到: '", cv_metrics_plot_file, "'\n")
# --- 7. 训练最终XGBoost模型 ---
cat("\n步骤5: 训练最终XGBoost模型 (使用确定的nrounds)...\n")
set.seed(123)
watchlist <- list(train = dtrain, eval = dvalidation) # 监控训练集和验证集
final_xgb_model <- NULL
training_history <- NULL # 用于存储训练过程中的指标
tryCatch({
# 使用 xgb.train 并捕获评估日志
# verbose=1 会打印评估结果, verbose=0 静默
# 如果想手动捕获，可以使用 callbacks = list(cb.evaluation.log())
# 但更简单的是在watchlist中指定，然后从 $evaluation_log 提取
final_xgb_model <- xgb.train(
params = params,
data = dtrain,
nrounds = best_nrounds,
watchlist = watchlist,
print_every_n = 10, # 每10轮打印一次
# early_stopping_rounds = if (best_nrounds > 50) EARLY_STOPPING_ROUNDS_CV else NULL, # 最终模型也可以用早停
verbose = 1
)
if (!is.null(final_xgb_model$evaluation_log)) {
training_history <- as.data.frame(final_xgb_model$evaluation_log)
}
}, error = function(e) {
cat("错误: 训练最终XGBoost模型时失败: ", conditionMessage(e), "\n")
final_xgb_model <<- NULL
})
if(is.null(final_xgb_model)) stop("最终XGBoost模型训练失败。")
cat("最终XGBoost模型训练完成。\n")
# 绘制训练过程中的指标曲线 (类似loss/accuracy曲线)
if (!is.null(training_history) && nrow(training_history) > 0) {
train_hist_melt <- melt(training_history, id.vars = "iter",
measure.vars = grep("^(train|eval)_(mlogloss|merror)$", names(training_history), value = TRUE),
variable.name = "metric_type", value.name = "value")
png(train_metrics_plot_file, width = 1200, height = 700)
p_train_hist <- ggplot(train_hist_melt, aes(x = iter, y = value, color = metric_type)) +
geom_line(linewidth = 1) +
labs(title = "XGBoost Training History (Metrics vs. Rounds)",
x = "Number of Rounds (Iteration)", y = "Metric Value") +
scale_color_manual(values = c("train_mlogloss" = "darkblue", "eval_mlogloss" = "blue",
"train_merror" = "darkred", "eval_merror" = "red"),
labels = c("Train mlogloss", "Validation mlogloss", "Train merror", "Validation merror")) +
theme_minimal() +
theme(legend.position = "top")
print(p_train_hist)
dev.off()
cat("XGBoost训练历史指标曲线图已保存到: '", train_metrics_plot_file, "'\n")
} else {
cat("警告: 未能获取XGBoost训练历史用于绘图。\n")
}
# --- 8. 在测试集上评估模型性能 (更全面) ---
cat("\n步骤6: 在测试集上评估XGBoost模型 (更全面)...\n")
# ... (预测和混淆矩阵部分同前) ...
if (nrow(test_set_df) > 0 && ncol(test_set_df) > 1) {
pred_probs_xgb_test <- predict(final_xgb_model, dtest, reshape = TRUE)
pred_labels_numeric_xgb_test <- max.col(pred_probs_xgb_test) - 1
pred_labels_factor_xgb_test <- numeric_map_to_original_factor[as.character(pred_labels_numeric_xgb_test)]
test_labels_factor_original <- numeric_map_to_original_factor[as.character(test_labels_numeric)]
pred_labels_factor_xgb_test <- factor(pred_labels_factor_xgb_test, levels = original_levels)
test_labels_factor_original <- factor(test_labels_factor_original, levels = original_levels)
if(length(pred_labels_factor_xgb_test) == length(test_labels_factor_original) && length(pred_labels_factor_xgb_test) > 0) {
conf_matrix_test_xgb <- confusionMatrix(pred_labels_factor_xgb_test, test_labels_factor_original)
cat("\n混淆矩阵 (测试集 - XGBoost模型):\n")
print(conf_matrix_test_xgb)
# 计算每个类别的 Precision, Recall, F1-score
precision_by_class <- diag(conf_matrix_test_xgb$table) / colSums(conf_matrix_test_xgb$table)
recall_by_class <- diag(conf_matrix_test_xgb$table) / rowSums(conf_matrix_test_xgb$table)
f1_by_class <- 2 * (precision_by_class * recall_by_class) / (precision_by_class + recall_by_class)
f1_by_class[is.na(f1_by_class)] <- 0 # 处理分母为0的情况
# Macro averages (简单平均)
macro_precision <- mean(precision_by_class, na.rm = TRUE)
macro_recall <- mean(recall_by_class, na.rm = TRUE)
macro_f1 <- mean(f1_by_class, na.rm = TRUE)
# Weighted averages (按支持度加权)
support_by_class <- rowSums(conf_matrix_test_xgb$table)
weighted_precision <- weighted.mean(precision_by_class, w = support_by_class, na.rm = TRUE)
weighted_recall <- weighted.mean(recall_by_class, w = support_by_class, na.rm = TRUE)
weighted_f1 <- weighted.mean(f1_by_class, w = support_by_class, na.rm = TRUE)
sink(performance_metrics_file)
cat("--- XGBoost Model Performance on Test Set (Direct Usage, Full Eval) ---\n\n")
# ... (模型参数等信息同前) ...
print(conf_matrix_test_xgb) # 包含caret计算的byClass指标
cat("\n--- Manually Calculated Metrics (Test Set - XGBoost) ---\n")
cat("Precision by Class:\n"); print(precision_by_class)
cat("Recall by Class:\n"); print(recall_by_class)
cat("F1-Score by Class:\n"); print(f1_by_class)
cat("\nMacro-Averaged Precision: ", sprintf("%.4f", macro_precision), "\n")
cat("Macro-Averaged Recall:    ", sprintf("%.4f", macro_recall), "\n")
cat("Macro-Averaged F1-Score:  ", sprintf("%.4f", macro_f1), "\n")
cat("\nWeighted-Averaged Precision: ", sprintf("%.4f", weighted_precision), "\n")
cat("Weighted-Averaged Recall:    ", sprintf("%.4f", weighted_recall), "\n")
cat("Weighted-Averaged F1-Score:  ", sprintf("%.4f", weighted_f1), "\n")
cat("\n--- Caret Overall Statistics ---\n")
cat("Accuracy: ", sprintf("%.4f", conf_matrix_test_xgb$overall['Accuracy']), "\n")
cat("Kappa:    ", sprintf("%.4f", conf_matrix_test_xgb$overall['Kappa']), "\n")
sink()
cat("XGBoost测试集评估结果已保存到: '", performance_metrics_file, "'\n")
} else { cat("警告: XGBoost测试集预测或标签数据不一致或为空。\n") }
} else { cat("测试集为空或无特征，跳过XGBoost最终模型评估。\n")}
# --- 9. XGBoost特征重要性 --- (逻辑同前)
cat("\n步骤7: 提取、排序并可视化XGBoost模型的特征重要性...\n")
# ... (xgb.importance 和 xgb.plot.importance 逻辑不变) ...
tryCatch({
importance_matrix_xgb <- xgb.importance(feature_names = colnames(train_features_matrix), model = final_xgb_model)
if (!is.null(importance_matrix_xgb) && nrow(importance_matrix_xgb) > 0) {
cat("XGBoost特征重要性 (Top 20):\n"); print(head(importance_matrix_xgb, 20))
png(importance_plot_file, width = 1000, height = 700)
xgb.plot.importance(importance_matrix_xgb, top_n = min(20, nrow(importance_matrix_xgb)), main = "Top Important Features (XGBoost Direct)")
dev.off()
cat("XGBoost特征重要性图已保存到: '", importance_plot_file, "'\n")
} else { cat("警告: 未能从XGBoost模型中提取特征重要性。\n") }
}, error = function(e) { cat("错误: 提取XGBoost特征重要性时失败: ", conditionMessage(e), "\n") })
# --- 10. 保存训练好的XGBoost模型 ---
cat("\n步骤8: 保存训练好的XGBoost模型...\n")
# ... (xgb.save 或 saveRDS 逻辑不变) ...
xgb.save(final_xgb_model, fname = xgb_model_output_file)
cat("XGBoost模型已保存到: '", xgb_model_output_file, "'\n")
cat("\n--- 模型训练与评估 (直接使用xgboost, 增强评估) 完成！ ---\n")
